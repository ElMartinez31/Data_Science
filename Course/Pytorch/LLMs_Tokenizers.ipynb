{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEhhuL7MHO8IA2iVedmcQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Grelatif/Data_Science/blob/main/LLMs_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Exploration des Tokenizers Hugging Face\n",
        "\n",
        "# Installe la bibliothèque Transformers\n",
        "!pip install -q transformers\n",
        "\n",
        "# Imports\n",
        "from transformers import (\n",
        "    BertTokenizer, RobertaTokenizer, GPT2Tokenizer,\n",
        "    DistilBertTokenizer, T5Tokenizer, XLNetTokenizer, BartTokenizer\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "fwQdDBxT6siT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentences = [\n",
        "    \"I love Hugging Face and machine learning!\",\n",
        "    \"Transformers are amazing for NLP tasks.\",\n",
        "    \"Short sentence.\",\n",
        "    \"Another example to explore tokenizers.\",\n",
        "    \"How do we handle different languages?\"\n",
        "]"
      ],
      "metadata": {
        "id": "lQQcEOX56sfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(\"BERT Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)# tokens de la phrase de base\n",
        "    encoded = tokenizer(sentence)#encode la phrase en un dictionnaire contenant plusieurs informations (inputs_id, attention_mask, token_type_id, mais pas les tokens)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# token_type_id :\n",
        "# token_type_ids sont des identifiants de type de token qui sont utilisés pour indiquer à BERT\n",
        "# quels tokens appartiennent à quelle partie du texte, notamment dans les cas où tu as\n",
        "# deux segments de texte (par exemple, une question et sa réponse).\n"
      ],
      "metadata": {
        "id": "ve0h0ecb6sdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "encoded[\"attention_mask\"]\n",
        "encoded.get('attention_mask', 'N/A')"
      ],
      "metadata": {
        "id": "I7eLpTfp6sap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa Tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "print(\"RoBERTa Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "VnxH_l9Q6sYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** GPT2: **\n",
        "\n",
        "GPT-2 est un modèle de langage auto-régressif, ce qui signifie qu'il génère du texte un token à la fois. Il prend en entrée une séquence de tokens et génère le prochain token de manière séquentielle. Il n'a pas besoin de token de début ([CLS]) ou de séparateur ([SEP]) parce qu'il génère du texte dans une seule séquence continue.\n",
        "\n",
        "Contrairement à BERT, qui est bidirectionnel et a été formé avec des masques de tokens pour des tâches comme la classification de texte ou la prédiction de masques, GPT-2 génère simplement du texte à partir d'une séquence d'entrée et a besoin de contextualiser chaque token en fonction des précédents.\n",
        "\n",
        " \"Ġ\" représente un espace dans la tokenisation de GPT-2. C'est un token spécial qui indique où un mot commence après un espace.\n",
        "\n"
      ],
      "metadata": {
        "id": "-hIKySpB7BiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-2 Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "print(\"GPT-2 Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n",
        "    encoded\n"
      ],
      "metadata": {
        "id": "7w88YJ1O6sUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DistillBert Tokenizer**\n",
        "\n",
        "Tres similaire à celui de BERT"
      ],
      "metadata": {
        "id": "kgOMNTcB7I40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DistilBERT Tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "print(\"DistilBERT Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "J5jT6-JY6sSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** T5 Tokenizer**\n",
        "\n",
        "Le T5 tokenizer utilise une méthode de tokenisation appelée SentencePiece.\n",
        "\n",
        "SentencePiece est une approche qui découpe le texte en sous-mots (similaire à WordPiece), mais contrairement à WordPiece, il n'a pas besoin d'un vocabulaire préexistant. SentencePiece apprend son vocabulaire directement à partir des données et peut également gérer des unités de caractères.\n",
        "\n",
        "SentencePiece fonctionne sur l'idée que les mots ou les caractères peuvent être des unités atomiques (par exemple, une unité pourrait être un caractère, une racine de mot ou un mot entier).\n",
        "\n",
        "Exemple : Le mot \"unhappiness\" pourrait être découpé en [\"▁un\", \"happiness\"], où \"▁\" représente un espace dans le vocabulaire de SentencePiece.\n",
        "\n"
      ],
      "metadata": {
        "id": "6P8rvk8M7NuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T5 Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "print(\"T5 Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "6l75Boh86sP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** XLnet **\n",
        "\n",
        "XLNet combine les caractéristiques des modèles autoregressifs (comme GPT) et des modèles bidirectionnels (comme BERT).\n",
        "\n",
        "BERT apprend à prédire des tokens masqués dans une séquence en utilisant un apprentissage bidirectionnel, ce qui signifie qu'il regarde à la fois les tokens à gauche et à droite du token masqué.\n",
        "\n",
        "XLNet n'utilise pas de masquage traditionnel comme BERT. Au lieu de cela, il utilise une approche de permutation autoregressive, où il apprend à prédire chaque token en fonction des autres tokens dans une permutation aléatoire de la séquence.\n",
        "\n",
        "Cela signifie qu'XLNet permet au modèle de capturer des relations contextuelles à la fois de gauche à droite et de droite à gauche sans avoir besoin d'un token de type [CLS] ou [SEP], comme dans BERT."
      ],
      "metadata": {
        "id": "m2Ac-K9C7VGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZYA8GchL7Yo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XLNet Tokenizer\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "print(\"XLNet Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "G1xGbm6n6sNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bart**\n",
        "\n",
        "Tokenisation avec BPE : BART utilise un vocabulaire basé sur BPE, ce qui permet de découper les mots en sous-mots.\n",
        "\n",
        "BPE est une méthode similaire à WordPiece et SentencePiece dans le sens où elle découpe des mots en sous-mots pour les rendre plus manipulables par le modèle, mais l'algorithme d'apprentissage est légèrement différent.\n",
        "\n",
        "BPE cherche à maximiser la fréquence des paires de caractères dans un texte, et en combinant ces paires, le vocabulaire se construit de manière dynamique.\n"
      ],
      "metadata": {
        "id": "GT0_rEmh7cqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  BART Tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "\n",
        "print(\"BART Tokenizer\")\n",
        "for sentence in sentences:\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    encoded = tokenizer(sentence)\n",
        "    decoded = tokenizer.decode(encoded['input_ids'])\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "    print(f\"Attention Mask: {encoded.get('attention_mask', 'N/A')}\")\n",
        "    print(f\"Decoded: {decoded}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "bKivZY256sLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Encoding avec Padding et Troncature\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "batch = tokenizer.batch_encode_plus(\n",
        "    sentences,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(\"Batch Encoding\")\n",
        "print(batch)\n"
      ],
      "metadata": {
        "id": "_muTtmt56sIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comparaison du nombre de tokens pour chaque tokenizer dans une liste de phrases\n",
        "tokenizers = {\n",
        "    \"BERT\": BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
        "    \"RoBERTa\": RobertaTokenizer.from_pretrained(\"roberta-base\"),\n",
        "    \"GPT-2\": GPT2Tokenizer.from_pretrained(\"gpt2\"),\n",
        "    \"T5\": T5Tokenizer.from_pretrained(\"t5-small\"),\n",
        "}\n",
        "\n",
        "print(\"🔍 Comparaison des tokens pour chaque tokenizer:\")\n",
        "for name, tokenizer in tokenizers.items():\n",
        "    for sentence in sentences:\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        print(f\"{name} -> Sentence: '{sentence}' -> {len(tokens)} tokens\")\n",
        "        print(f\"{name} -> tokens: {tokens}\")\n",
        "\n",
        "\n",
        "# QUite a lot of differences, and usecases.\n"
      ],
      "metadata": {
        "id": "0N9C7D7W6sGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # let's wrap up all the info we saw\n",
        "\n",
        "# Modèle\t    Tokenizer\t          Méthode\t    Tokens spéciaux (exemples)\t    Type de modèle\t          Utilisation typique\n",
        "# BERT\t      BertTokenizer\t      WordPiece\t    [CLS], [SEP], [PAD], [MASK]\t  Encodeur\t                Classification, QA, NER\n",
        "# DistilBERT\tDistilBertTokenizer\tWordPiece\t    [CLS], [SEP], [PAD], [MASK]\t  Encodeur                  (light BERT)\tIdem BERT, + rapide\n",
        "# RoBERTa\t    RobertaTokenizer\t  BPE\t          <s>, </s>, <pad>, <mask>\t    Encodeur\t                Idem BERT, + robuste\n",
        "# XLNet\t      XLNetTokenizer\t    WordPiece\t    [CLS], [SEP], [PAD], [MASK]\t  Permuté + autoregressif \tQA, texte long, perf. élevée\n",
        "# GPT-2\t      GPT2Tokenizer\t      BPE\tAucun     [CLS]/[SEP], pas de padding\t  Décodeur autoregressif\t  Génération de texte\n",
        "# BART\t      BartTokenizer\t      BPE\t          <s>, </s>, <pad>, <mask>\t    Encodeur-décodeur\t        Résumé, traduction, génération\n",
        "# T5\t        T5Tokenizer\t        SentencePiece\t<pad>, </s>, <unk>\t          Encodeur-décodeur\t        Tâches text-to-text\n",
        "\n"
      ],
      "metadata": {
        "id": "N9T8xMR76sC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fJi7hGqg6sAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJuxT5P-6r-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8p2Rf3Wp6r77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wr05fe3u6r5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HaAq1KPn6r2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ppHVFtc6qHl"
      },
      "outputs": [],
      "source": []
    }
  ]
}
