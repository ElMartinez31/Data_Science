{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKAsPNZMka3zFCOhgTEruk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElMartinez31/Data_Science/blob/main/Move_Pytorch_To_Production.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mettre en production un mod√®le entra√Æn√©\n",
        "\n",
        "Transformer un prototype (Jupyter Notebook, script de recherche) en une application scalable utilisable par des utilisateurs ou des syst√®mes.\n",
        "\n",
        "Exemple : Un mod√®le RL entra√Æn√© pour jouer √† Mario pourrait √™tre d√©ploy√© dans un jeu autonome ou un robot.\n",
        "\n",
        "Optimiser les performances\n",
        "\n",
        "R√©duire la latence et la consommation m√©moire (via TorchScript, ONNX, ou la quantification).\n",
        "\n",
        "Exemple : Un mod√®le PyTorch converti en TorchScript pour fonctionner sur mobile.\n",
        "\n",
        "Compatibilit√© avec diff√©rents environnements\n",
        "\n",
        "Faire fonctionner le mod√®le sur des serveurs, edge devices (t√©l√©phones, Raspberry Pi), ou le cloud (AWS, GCP).\n",
        "\n",
        "Int√©gration avec des pipelines industrielles\n",
        "\n",
        "Connecter le mod√®le √† des APIs (FastAPI, Flask), des bases de donn√©es, ou des outils comme Docker/Kubernetes.\n",
        "\n"
      ],
      "metadata": {
        "id": "j1XZwuBcv8nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üöÄONNX:\n",
        "\n",
        "Interop√©rabilit√© : Exporter un mod√®le PyTorch vers ONNX permet de l'utiliser avec :\n",
        "\n",
        "TensorFlow, MXNet, Scikit-learn (via onnxruntime)\n",
        "\n",
        "Acc√©l√©rateurs mat√©riels (NVIDIA TensorRT, Intel OpenVINO)\n",
        "\n",
        "Mobiles (Android/iOS via ONNX Runtime)\n",
        "\n",
        "Optimisation : ONNX permet des optimisations (fusion d'op√©rations, quantification) pour des inf√©rences plus rapides.\n",
        "\n",
        "Portabilit√© : Le fichier .onnx est autonome et contient tout le graphe de calcul."
      ],
      "metadata": {
        "id": "Q_yAmO0LxuRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemple : Exporter un mod√®le PyTorch en ONNX"
      ],
      "metadata": {
        "id": "EKw55DcnzGtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# D√©finition d'un mod√®le simple\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.fc = nn.Linear(10, 2)  # Couche lin√©aire (10 entr√©es, 2 sorties)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "model = SimpleModel()\n",
        "model.eval()  # Mode √©valuation (important pour l'export ONNX)"
      ],
      "metadata": {
        "id": "1j0omv_XwLkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export en ONNX\n",
        "\n",
        "# Exemple d'input (batch_size=1, input_dim=10)\n",
        "dummy_input = torch.randn(1, 10)\n",
        "\n",
        "# Export ONNX\n",
        "torch.onnx.export(\n",
        "    model,                     # Mod√®le PyTorch\n",
        "    dummy_input,               # Input exemple\n",
        "    \"simple_model.onnx\",       # Nom du fichier de sortie\n",
        "    input_names=[\"input\"],     # Nom de l'input\n",
        "    output_names=[\"output\"],   # Nom de l'output\n",
        "    dynamic_axes={\n",
        "        \"input\": {0: \"batch_size\"},  # Axe dynamique (taille de batch variable)\n",
        "        \"output\": {0: \"batch_size\"},\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "#torch.onnx.export: convertit le mod√®le en fichier .onnx.\n",
        "#dummy_input: est un exemple de tensor pour tracer le mod√®le.\n",
        "#dynamic_axes: permet de sp√©cifier des dimensions variables (utile pour des batchs de tailles diff√©rentes)."
      ],
      "metadata": {
        "id": "7rMNQQxxwMX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Charger et utiliser un mod√®le ONNX"
      ],
      "metadata": {
        "id": "QMSBITaW0EVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "\n",
        "# Cr√©ation d'un session ONNX Runtime\n",
        "ort_session = ort.InferenceSession(\"simple_model.onnx\")\n",
        "\n",
        "# Input (doit correspondre au format attendu)\n",
        "input_data = dummy_input.numpy()\n",
        "\n",
        "# Inf√©rence\n",
        "outputs = ort_session.run(\n",
        "    None,  # None car on veut toutes les sorties\n",
        "    {\"input\": input_data},\n",
        ")\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "UJ1ng8pGwMV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V√©rifier l'export ONNX"
      ],
      "metadata": {
        "id": "pEcSSGRj0jiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "model_onnx = onnx.load(\"simple_model.onnx\")\n",
        "onnx.checker.check_model(model_onnx)  # V√©rifie que le mod√®le est valide\n",
        "print(onnx.helper.printable_graph(model_onnx.graph))  # Affiche l'architecture"
      ],
      "metadata": {
        "id": "N6phkk_-wMTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cas pratique : Exporter un CNN (ex: ResNet)"
      ],
      "metadata": {
        "id": "uQnNMzH50wn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "# Charger ResNet-18 pr√©-entra√Æn√©\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Exemple d'input (3 canaux, 224x224)\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# Export ONNX\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"resnet18.onnx\",\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        ")"
      ],
      "metadata": {
        "id": "lhwHHdZz0waf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastAPI et ONNX :\n",
        "\n",
        "Un duo puissant pour le d√©ploiement de mod√®les ML\n",
        "En tant que data scientist, la combinaison de FastAPI et ONNX peut consid√©rablement optimiser votre pipeline de d√©ploiement de mod√®les. Voici comment ces deux technologies interagissent :\n",
        "\n",
        "üîó Relation entre FastAPI et ONNX\n",
        "FastAPI et ONNX sont compl√©mentaires mais servent des objectifs diff√©rents :\n",
        "\n",
        "FastAPI : Framework pour cr√©er des APIs web performantes (couche de service)\n",
        "\n",
        "ONNX (Open Neural Network Exchange) : Format ouvert pour repr√©senter des mod√®les ML (couche d'inf√©rence)\n",
        "\n",
        "üöÄ Pourquoi utiliser ONNX avec FastAPI ?\n",
        "Interop√©rabilit√© :\n",
        "\n",
        "ONNX permet d'exporter des mod√®les depuis diff√©rents frameworks (PyTorch, TensorFlow, scikit-learn)\n",
        "\n",
        "FastAPI expose ces mod√®les via une API standardis√©e\n",
        "\n",
        "Performance optimis√©e :\n",
        "\n",
        "Les mod√®les ONNX s'ex√©cutent plus vite gr√¢ce √† des optimisations sp√©cifiques\n",
        "\n",
        "FastAPI g√®re efficacement les requ√™tes entrantes\n",
        "\n",
        "D√©ploiement multiplateforme :\n",
        "\n",
        "ONNX fonctionne sur CPU/GPU et diff√©rents environnements\n",
        "\n",
        "FastAPI fournit une interface REST ind√©pendante de la plateforme"
      ],
      "metadata": {
        "id": "XQd34Gxh2-5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemple concret d'int√©gration"
      ],
      "metadata": {
        "id": "6PcFenze3YSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Chargement du mod√®le ONNX\n",
        "sess = ort.InferenceSession(\"modele.onnx\")\n",
        "input_name = sess.get_inputs()[0].name\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(input_data: list):\n",
        "    \"\"\"Endpoint pour les pr√©dictions ONNX\"\"\"\n",
        "    # Conversion des donn√©es d'entr√©e\n",
        "    input_array = np.array(input_data, dtype=np.float32)\n",
        "\n",
        "    # Inf√©rence ONNX\n",
        "    outputs = sess.run(None, {input_name: input_array})\n",
        "\n",
        "    return {\"prediction\": outputs[0].tolist()}"
      ],
      "metadata": {
        "id": "-fzpAiEowMQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workflow typique Data Science avec ONNX + FastAPI\n",
        "\n",
        "Entra√Ænement :\n",
        "\n",
        "D√©veloppez votre mod√®le dans PyTorch/TensorFlow/scikit-learn\n",
        "\n",
        "Conversion vers ONNX :\n",
        "\n",
        "python\n",
        "Copy\n",
        "torch.onnx.export(model, dummy_input, \"modele.onnx\")\n",
        "D√©ploiement avec FastAPI :\n",
        "\n",
        "Cr√©ez une API autour du mod√®le ONNX\n",
        "\n",
        "B√©n√©ficiez des performances accrues d'ONNX Runtime\n",
        "\n",
        "Consommation :\n",
        "\n",
        "L'API peut √™tre appel√©e par des applications web/mobiles\n",
        "\n",
        "‚ö° Avantages cl√©s de cette combinaison\n",
        "Latence r√©duite : Jusqu'√† 10x plus rapide qu'un mod√®le Python natif\n",
        "\n",
        "Compatibilit√© √©tendue : Fonctionne m√™me avec des mod√®les entra√Æn√©s sur d'autres plateformes\n",
        "\n",
        "√âconomie de ressources : Moins de CPU/m√©moire utilis√©s\n",
        "\n",
        "Maintenance simplifi√©e : Un seul format de mod√®le √† g√©rer\n",
        "\n",
        "üõ†Ô∏è Outils compl√©mentaires utiles\n",
        "ONNX Runtime : Moteur d'ex√©cution optimis√© pour les mod√®les ONNX\n",
        "\n",
        "Hummingbird : Convertit les mod√®les sklearn en ONNX\n",
        "\n",
        "Docker : Pour containeriser votre API FastAPI + mod√®le ONNX\n",
        "\n",
        "Cette combinaison est particuli√®rement utile quand vous avez besoin :\n",
        "\n",
        "De performances √©lev√©es en production\n",
        "\n",
        "De d√©ployer des mod√®les sur diff√©rentes plateformes\n",
        "\n",
        "D'une solution standardis√©e pour servir diff√©rents types de mod√®les\n",
        "\n"
      ],
      "metadata": {
        "id": "8TyRrB-k36PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jHNZhm4bwMOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deployer un modele Pytorch to Fast API\n"
      ],
      "metadata": {
        "id": "v46xHtDe4oIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "# 1. Charger ResNet-18 pr√©-entra√Æn√©\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "model.eval()  # Mode √©valuation\n",
        "\n",
        "# 2. Exemple de donn√©es (batch=1, RGB, 224x224)\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# 3. Test du mod√®le avant export\n",
        "output = model(dummy_input)\n",
        "print(\"Classe pr√©dite:\", torch.argmax(output, dim=1).item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hai1WVXXwMMd",
        "outputId": "11e2d15f-6759-47f0-ea58-66c74ebdf0f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 111MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classe pr√©dite: 107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Th√©orie ONNX :\n",
        "\n",
        "ONNX a besoin d'un exemple d'input pour :\n",
        "\n",
        "Tracer le graphe de calcul\n",
        "\n",
        "D√©terminer les shapes des tensors interm√©diaires\n",
        "\n",
        "Valider que toutes les op√©rations sont support√©es"
      ],
      "metadata": {
        "id": "TAcOcPRu9wEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Export ONNX avec batch dynamique\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"resnet18.onnx\",\n",
        "    input_names=[\"input\"],\n",
        "    output_names=[\"output\"],\n",
        "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
        "    opset_version=11\n",
        ")\n"
      ],
      "metadata": {
        "id": "bXy9YAtuwMKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce qui se passe pendant l'export :\n",
        "\n",
        "Tracing :\n",
        "\n",
        "PyTorch ex√©cute le mod√®le avec dummy_input\n",
        "\n",
        "Enregistre toutes les op√©rations effectu√©es\n",
        "\n",
        "Cr√©e un graphe de calcul (DAG)\n",
        "\n",
        "Validation :\n",
        "\n",
        "V√©rifie que toutes les op√©rations sont support√©es par ONNX\n",
        "\n",
        "Certaines op√©rations PyTorch complexes peuvent n√©cessiter des adaptations\n",
        "\n",
        "S√©rialisation :\n",
        "\n",
        "Le graphe + poids sont sauvegard√©s au format protobuf (.onnx)"
      ],
      "metadata": {
        "id": "V3mVjTfr-CRo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Points d'attention importants :\n",
        "\n",
        "Compatibilit√© des op√©rations :\n",
        "\n",
        "Certaines couches PyTorch n'ont pas d'√©quivalent direct ONNX\n",
        "\n",
        "Solution : r√©impl√©menter avec des op√©rations de base\n",
        "\n",
        "Contr√¥le de flux :\n",
        "\n",
        "Les boucles/conditions natives Python ne sont pas exportables\n",
        "\n",
        "Utiliser torch.jit.script pour les mod√®les avec logique complexe\n",
        "\n",
        "Shape Inference :\n",
        "\n",
        "ONNX doit pouvoir d√©duire toutes les shapes interm√©diaires\n",
        "\n",
        "Probl√®mes fr√©quents avec les op√©rations de reshape dynamique"
      ],
      "metadata": {
        "id": "FuzuAuC9-TEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "# Charger le mod√®le\n",
        "onnx_model = onnx.load(\"resnet18.onnx\")\n",
        "\n",
        "# Valider le sch√©ma\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Afficher le graphe\n",
        "print(onnx.helper.printable_graph(onnx_model.graph))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "UpBNF40N-Xid",
        "outputId": "3f5bf725-50f7-48f3-cbdb-62715169859b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'onnx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-11054f681d67>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Charger le mod√®le\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0monnx_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resnet18.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnx'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cr√©er une API avec FastAPI"
      ],
      "metadata": {
        "id": "3jyrthW49Rcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install fastapi\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCF3tTc2-709",
        "outputId": "195eab38-0de9-4f5a-d5d1-4aef84a655dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, File, UploadFile\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "from PIL import Image\n",
        "import io\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "# Charger le mod√®le ONNX\n",
        "sess = ort.InferenceSession(\"resnet18.onnx\")\n",
        "\n",
        "def preprocess_image(image_bytes):\n",
        "    img = Image.open(io.BytesIO(image_bytes))\n",
        "    img = img.resize((224, 224))  # Resize selon le mod√®le\n",
        "    img = np.array(img).transpose(2, 0, 1)  # CHW format\n",
        "    img = img.astype(np.float32) / 255.0  # Normaliser\n",
        "    img = np.expand_dims(img, axis=0)  # Ajouter batch dimension\n",
        "    return img\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "async def predict(file: UploadFile = File(...)):\n",
        "    # Lire l'image upload√©e\n",
        "    image_bytes = await file.read()\n",
        "\n",
        "    # Pr√©traiter l'image\n",
        "    input_tensor = preprocess_image(image_bytes)\n",
        "\n",
        "    # Faire la pr√©diction\n",
        "    outputs = sess.run(\n",
        "        None,\n",
        "        {\"input\": input_tensor}\n",
        "    )\n",
        "\n",
        "    # Traiter les outputs (ex: obtenir la classe pr√©dite)\n",
        "    predicted_class = np.argmax(outputs[0])\n",
        "\n",
        "    return {\"predicted_class\": int(predicted_class)}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OXmAWd9XwMHe",
        "outputId": "c161972e-924a-4c9d-d722-7d5cca29d024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ab15aaa15cb4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet18-v1-7.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"resnet18.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Charger le mod√®le ONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_splittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McALf3DrwME-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KGEKE7MHwMCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lN5da0EVwMAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CNDJXlvfwL9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HTtc9xpVwL7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KuUjVI_3wL4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyMo-qrmv70m"
      },
      "outputs": [],
      "source": []
    }
  ]
}
