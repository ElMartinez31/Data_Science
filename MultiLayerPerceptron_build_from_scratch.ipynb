{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"19S8YZjMcCJMuSMdzPETlUoGdZLRq9_aY","authorship_tag":"ABX9TyMSH3abXpA0m3wAqXI+AKz4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[" We are going to build a MLP relatively simple, but without using Keras or TF, coding our own tools to do it, in order to fully understand what's behind.\n","\n"," 3 dimesions entry\n"," 1 hidden layer with 2 neurons\n"," 1 output (binary classif)\n"," Relu and Sigmoid as activation functions.\n"," MSE as Loss function"],"metadata":{"id":"WWW0qk5nw4c5"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import random\n"],"metadata":{"id":"KzbWI5HhyZfV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6K2SIKVIwUrq","executionInfo":{"status":"ok","timestamp":1740504078037,"user_tz":-60,"elapsed":73,"user":{"displayName":"Martin D","userId":"05485540402824828302"}},"outputId":"2d73eac1-c4b1-45e4-8799-abef051bd4af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.24061956601979217\n","Epoch 100, Loss: 0.1017810053666683\n","Epoch 200, Loss: 0.0298923598931502\n","Epoch 300, Loss: 0.014349203383598096\n","Epoch 400, Loss: 0.00890701244973803\n","Epoch 500, Loss: 0.00630130368467822\n","Epoch 600, Loss: 0.004813478701372741\n","Epoch 700, Loss: 0.0038637582697041957\n","Epoch 800, Loss: 0.003211078173272466\n","Epoch 900, Loss: 0.0027375970347008442\n"]}],"source":["# Initialize\n","\n","# N = 5 Examples (batch size)\n","# d = 3, input dim\n","# o = 1, output dim\n","# h = 2, neurons in hidden layer\n","\n","# training epochs\n","epochs = 1000\n","\n","# Input (N,d)\n","X = np.array([[0.1, 0.5, 0.3],\n","              [0.4, 0.2, -0.7],\n","              [0.7, 0.8, 0.2],\n","              [-0.3, 0.9, 0.1],\n","              [0.6, 0.3, -0.4]])\n","\n","# output (N,o)\n","Y = np.array([[1], [0], [1], [1], [0]]) # !!!very important to be in this format (5,1) and not the (5,) given by Y = np.array([1, 0, 1, 1, 0])\n","\n","# Weights (d,h)\n","# Initialisation correcte des poids et biais\n","W1 = np.random.rand(3, 2)  # d=3, h=2\n","B1 = np.random.rand(1, 2)  # (1, 2) pour que le biais soit diffusé sur chaque exemple\n","\n","\n","\n","# First activation fun\n","def relu(z):\n","  return np.maximum(z,0) #np.maximum apply the ReLU activation element-wise across the matrix, max doesn't\n","\n","# Propagation avant (forward propagation)\n","Z1 = np.dot(X, W1) + B1  # X: (5, 3), W1: (3, 2), B1: (1, 2)\n","A1 = relu(Z1)  # A1: (5, 2)\n","\n","\n","W2 = np.random.rand(2, 1)  # h=2, o=1\n","B2 = np.random.rand(1, 1)  # (1, 1) pour le biais de sortie\n","\n","Z2 = np.dot(A1, W2) + B2  # A1: (5, 2), W2: (2, 1), B2: (1, 1)\n","\n","\n","\n","# Activation func 2 , sigmoid\n","def sigmoid(z):\n","  return (1/(1+ np.exp(-z))) #NumPy applique l'opération élément par élément (element-wise) à toute la matrice ou le vecteur, donc pas de pb ici\n","\n","A2 = sigmoid(Z2)  # A2: (5, 1)\n","\n","E = Y - A2 # Brute error\n","\n","# Loss function\n","def loss(Y_true,Y_pred):\n","  N = Y_true.shape[0]\n","  compute = (1/N)*np.sum((Y_true - Y_pred)**2) # np.mean() does the exact same\n","  return compute\n","\n","\n","\n","\n","# So now we have the complete architecture of our neural network\n","# we need to train it on epochs = 1000 for example.\n","\n","for epoch in range(epochs):\n","\n","  Z1 = np.dot(X, W1) + B1\n","  A1 = relu(Z1)\n","  Z2 = np.dot(A1, W2) + B2\n","  A2 = sigmoid(Z2)\n","  loss_value = loss(Y, A2)\n","  # Let's compute the backpropagation\n","  # We already computed the formulas thanks to chain partial derivatives\n","\n","  # dLoss/dB2\n","  dLoss_dB2 = np.sum((A2 - Y) * (A2 * (1 - A2)), axis=0, keepdims=True) # axis = 0 gives the sum along the rows,\n","  #meaning the results at the end of each column, keeping the number of columns so the result is\n","  # [sum C1, sum C2] instead of the global sum\n","\n","\n","  #dLoss/dW2\n","  dLoss_dW2 = np.dot(A1.T, (A2 - Y) * (A2 * (1 - A2)))\n","\n","\n","  #dLoss/dB1\n","  dLoss_dB1 = np.sum(((A2 - Y) * (A2 * (1 - A2))) @ W2.T * (Z1 > 0), axis=0, keepdims=True)\n","\n","\n","  #dLoss/dW1\n","  dLoss_dW1 = np.dot(X.T, ((A2 - Y) * (A2 * (1 - A2))) @ W2.T * (Z1 > 0))\n","\n","  # at each epochs we update the parameters\n","  learning_rate = 0.1\n","\n","  # Correct going in the opposite direction of the gradient\n","  B1 -= learning_rate * dLoss_dB1\n","  B2 -= learning_rate * dLoss_dB2\n","  W1 -= learning_rate * dLoss_dW1\n","  W2 -= learning_rate * dLoss_dW2\n","\n","  # Affichage de la perte toutes les 100 époques\n","  if epoch % 100 == 0:\n","      print(f\"Epoch {epoch}, Loss: {loss_value}\")\n","\n"]},{"cell_type":"markdown","source":["Entrée : 3 neurones (dimensions des entrées).\n","Deux couches cachées : 1ère couche avec 4 neurones, 2ème couche avec 3 neurones.\n","Sortie : 2 neurones (au lieu d'une sortie binaire, on suppose ici une sortie pour une classification multi-classe).\n","Activation : ReLU pour les couches cachées, Softmax pour la sortie (cas de classification multi-classe)."],"metadata":{"id":"3qGlUJn5rDJF"}},{"cell_type":"code","source":["\n","#On est dans une classif multiclasse (2 classes) ce qui est différent d'une classif binaire. En effet la sortie est forcément\n","#du type Y = [[1,0],[1,0],[1,0],[0,1],[1,0],[0,1]]   [1,0] et [0,1] sont les encodages oneHot de 1 et 0, nos deux classes.\n","\n","\n","########## Pourquoi Softmax exige un encodage one-hot ([1,0] et [0,1]) et empeche l'utilisation plus simple binaire de 0 ou 1  ?\n","\n","#Si on a deux classes, la sortie du réseau sera un vecteur à 2 dimensions, comme ceci :\n","\n","#A3 = softmax(Z3)\n","#où A3 pourrait donner un résultat comme :\n","#[[0.85, 0.15],  # Probabilité d'appartenir à la classe 0 = 85%, classe 1 = 15%\n","# [0.20, 0.80],  # Probabilité d'appartenir à la classe 0 = 20%, classe 1 = 80%\n","# ...]\n","#Dans ce cas, la classe prédite est l'indice du plus grand score (argmax(A3, axis=1)).\n","\n","#Pourquoi on encode les labels (Y) en one-hot ([1,0] et [0,1]) ?\n","\n","#Parce que Softmax prédit un vecteur et la perte de cross-entropie catégorielle (categorical_crossentropy) attend un vecteur de même dimension\n","#pour comparer chaque probabilité.\n","#Problème si Y est encodé avec [0] et [1] au lieu de one-hot :\n","#Avec Softmax, Y doit avoir la même forme que la sortie du réseau (A3).\n","#Si Y était sous la forme :\n","#Y = np.array([[0], [1], [1], [0], [1]])\n","#alors la perte essayerait de comparer un vecteur (A3 de shape (N,2)) avec un scalaire (Y de shape (N,1)), ce qui ne fonctionnerait pas.\n","\n"],"metadata":{"id":"T8w6AewTwttX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N = 10\n","d = 3\n","h1 = 4\n","h2 = 3\n","o = 2\n","\n","def relu(z):\n","  return np.maximum(0,z)\n","\n","def softmax(z):\n","    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # stabilité numérique\n","    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","\n","# X (N,d) = (5,3)\n","X = np.array(np.random.rand(5,3))\n","\n","#Y (N,o) = (5,2)\n","Y = np.array([[1,0],\n","              [0,1],\n","              [0,1],\n","              [0,1],\n","              [1,0],])\n","# W1 (d,h1)\n","W1 = np.random.rand(d,h1)\n","# B1 (1,h1)\n","B1 = np.random.rand(1,h1)\n","\n","#W2 (h1,h2)\n","W2 = np.random.rand(h1,h2)\n","#B2 (1,h2)\n","B2 = np.random.rand(1,h2)\n","\n","#W3 (h2,o)\n","W3 = np.random.rand(h2, o)\n","#B3 (1,o)\n","B3 = np.random.rand(1,o)\n","\n","# First hidden layer\n","Z1 = np.dot(X,W1) + B1\n","A1 = relu(Z1)\n","\n","#Second\n","Z2 = np.dot(A1,W2) + B2\n","A2 = relu(Z2)\n","\n","# Output classifiction\n","Z3 = np.dot(A2, W3) + B3\n","A3 = softmax(Z3)\n","\n","Error = Y - A3\n","\n","def categorical_crossentropy(Y, A): # Check l'exemple apres le training\n","    return -np.mean(np.sum(Y * np.log(A + 1e-9), axis=1))  # Ajout de 1e-9 pour éviter log(0)\n","\n"],"metadata":{"id":"AP3rHWNVwtvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entraînement du réseau\n","epochs = 1000\n","learning_rate = 0.1\n","for epoch in range(epochs):\n","    # Forward propagation\n","    Z1 = np.dot(X, W1) + B1\n","    A1 = relu(Z1)\n","    Z2 = np.dot(A1, W2) + B2\n","    A2 = relu(Z2)\n","    Z3 = np.dot(A2, W3) + B3\n","    A3 = softmax(Z3)\n","\n","    # Calcul de la perte\n","    loss_value = categorical_crossentropy(Y, A3)\n","\n","    # Backpropagation\n","    # La seul calcul vraiment difficile est celui de la dérivée de la categorical crossentropy\n","    # heuresuement, elle se simplifie avec la softmax (et A3 = softmax(Z3)), d'où:\n","    dLoss_dZ3 = A3 - Y # La fonction de perte utilisée est l'entropie croisée avec le softmax. Le gradient de la perte par rapport à\n","#Z3 est assez simple grâce à la propriété du softmax combinée avec l'entropie croisée => A3 - Y\n","    dLoss_dW3 = np.dot(A2.T, dLoss_dZ3) # dloss/dz3 .dZ3/dw3\n","    dLoss_dB3 = np.sum(dLoss_dZ3, axis=0, keepdims=True)\n","\n","    dLoss_dZ2 = np.dot(dLoss_dZ3, W3.T) * (Z2 > 0)\n","    dLoss_dW2 = np.dot(A1.T, dLoss_dZ2)\n","    dLoss_dB2 = np.sum(dLoss_dZ2, axis=0, keepdims=True)\n","\n","    dLoss_dZ1 = np.dot(dLoss_dZ2, W2.T) * (Z1 > 0)\n","    dLoss_dW1 = np.dot(X.T, dLoss_dZ1)\n","    dLoss_dB1 = np.sum(dLoss_dZ1, axis=0, keepdims=True)\n","\n","    # Mise à jour des poids et biais\n","    W1 -= learning_rate * dLoss_dW1\n","    B1 -= learning_rate * dLoss_dB1\n","    W2 -= learning_rate * dLoss_dW2\n","    B2 -= learning_rate * dLoss_dB2\n","    W3 -= learning_rate * dLoss_dW3\n","    B3 -= learning_rate * dLoss_dB3\n","\n","    # Affichage de la perte toutes les 100 époques\n","    if epoch % 100 == 0:\n","        print(f\"Epoch {epoch}, Loss: {loss_value}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"955pEUV06cOx","executionInfo":{"status":"ok","timestamp":1740585035811,"user_tz":-60,"elapsed":77,"user":{"displayName":"Martin D","userId":"05485540402824828302"}},"outputId":"c4980978-bd3e-4a9d-92f3-cea3e95f8e4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 0.7119652245840109\n","Epoch 100, Loss: 0.8744412539910524\n","Epoch 200, Loss: 0.02072969118628506\n","Epoch 300, Loss: 0.004293450260957775\n","Epoch 400, Loss: 0.00183796248045954\n","Epoch 500, Loss: 0.0010669119867656202\n","Epoch 600, Loss: 0.0007217572689745832\n","Epoch 700, Loss: 0.000533488041412589\n","Epoch 800, Loss: 0.0004171166324759133\n","Epoch 900, Loss: 0.00033987055940390626\n"]}]},{"cell_type":"code","source":["#############  Exemple de calcul a la main avec la categorical_crossentropy\n","import numpy as np\n","\n","# Labels one-hot (3 exemples, 2 classes)\n","Y_test = np.array([\n","    [1, 0],  # Classe 0\n","    [0, 1],  # Classe 1\n","    [1, 0]   # Classe 0\n","])\n","\n","# Prédictions Softmax (probas pour chaque classe)\n","A = np.array([\n","    [0.9, 0.1],  # Probabilité de la classe 0: 90%, classe 1: 10%\n","    [0.3, 0.7],  # Probabilité de la classe 0: 30%, classe 1: 70%\n","    [0.8, 0.2]   # Probabilité de la classe 0: 80%, classe 1: 20%\n","])\n","\n","# Calcul de la categorical crossentropy\n","loss_values = -np.sum(Y_test * np.log(A), axis=1)  # Somme sur j (les classes)\n","loss_mean = np.mean(loss_values)  # Moyenne sur i (les exemples)\n","\n","#Y_test * np.log(A):\n","[[1, 0],     * log([[0.9, 0.1]])  → [log(0.9), 0]\n"," [0, 1],       log([[0.3, 0.7]])  → [0, log(0.7)]\n"," [1, 0]]       log([[0.8, 0.2]])  → [log(0.8), 0]\n","\n","#= [[-0.105, 0], [0, -0.357], [-0.223, 0]]\n","# ensuite On somme sur axis=1 (les classes) pour obtenir un vecteur (perte pour chaque exemple)  :\n","# -np.sum(Y_test * np.log(A), axis=1\n","#[-0.105, -0.357, -0.223]\n","#On fait la moyenne sur tous les exemples :\n","#-np.mean(np.sum(Y * np.log(A + 1e-9), axis=1))\n","#(-0.105 - 0.357 - 0.223) / 3 ≈ 0.228\n","\n"],"metadata":{"id":"yQ1Tsahywtx9"},"execution_count":null,"outputs":[]}]}