{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1a5R5gcw7qEPXS90fbSOk2ZBGZpFZzFBG",
      "authorship_tag": "ABX9TyNHdIEeWkJ9h3SVi3FEcof5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Grelatif/Data_Science/blob/main/Attention_Tokenizer_and_Dataset_build.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "base course:\n",
        "https://github.com/priyammaz/PyTorch-Adventures/blob/main/PyTorch%20for%20NLP/Seq2Seq%20for%20Neural%20Machine%20Translation/tokenizer.py"
      ],
      "metadata": {
        "id": "TFUP6FX4VEhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import argparse\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import NFC, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import decoders\n",
        "from tokenizers.processors import TemplateProcessing"
      ],
      "metadata": {
        "id": "h-66EdGg2Sti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create our own french tokeniezr\n",
        "\n",
        "#dict of special tokens\n",
        "special_token_dict = {\"unknown_token\": \"[UNK]\",\n",
        "                      \"pad_token\": \"[PAD]\",\n",
        "                      \"start_token\": \"[BOS]\",\n",
        "                      \"end_token\": \"[EOS]\"}\n",
        "\n",
        "def french_tokenizer(path_to_data_root):\n",
        "  tokenizer = Tokenizer(WordPiece()) #subwords tokenization (like for BERT)\n",
        "  tokenizer.normalizer = normalizers.Sequence([NFC(), Lowercase()])\n",
        "  tokenizer.pre_tokenizer = Whitespace()\n",
        "  #NFC(): Normalisation Unicode NFC (Normalization Form Canonical Composition).\n",
        "  #Elle convertit par exemple des caractères décomposés (e + accent) en leur forme composée (é).\n",
        "  #normalizers: Définit une séquence de normalisation du texte en entrée.\n",
        "  #pre_tokenizer: methode de pretokenization\n",
        "  #Whitespace: Coupe le texte d’entrée en tokens en se basant sur les espaces blancs (comme les espaces, tabulations, sauts de ligne)\n",
        "\n",
        "  # Find all Target Language Files (ours are french ending with fr) #\n",
        "  # Ces fichiers serviront à entraîner le tokenizer.\n",
        "  french_files = glob.glob(os.path.join(path_to_data_root, \"**/*.en\"), recursive=True)# google drive \"French_tokenizer\"\n",
        "\n",
        "  # Train Tokenizer #\n",
        "  # Définit le \"trainer\" pour WordPiece :\n",
        "  # - vocab_size : taille du vocabulaire final (32 000 sous-mots)\n",
        "  # - special_tokens : liste de tokens spéciaux comme [PAD], [CLS], etc., fournie par `special_token_dict`\n",
        "  trainer = WordPieceTrainer(vocab_size=32000, special_tokens=list(special_token_dict.values()))\n",
        "  # Entraîne le tokenizer sur les fichiers .fr collectés\n",
        "  tokenizer.train(french_files, trainer)\n",
        "  # Créer le répertoire si il n'existe pas\n",
        "  output_dir = \"trained_tokenizer\"\n",
        "  os.makedirs(output_dir, exist_ok=True)  # Crée le répertoire s'il n'existe pas\n",
        "  #save tokenizer\n",
        "  tokenizer.save(\"trained_tokenizer/french_wp.json\")\n",
        "\n",
        "  # Le tokenizer est mainetant fonctionnel\n",
        "\n",
        "\n",
        "# Nous allons maintenant créer une classe qui permet l'utilisation  pratique de ce tokenizer\n",
        "# Cette classe encapsule toutes les fonctionnalités utiles du tokenizer :\n",
        "# Encodage de phrases en ids.\n",
        "# Décodage d’ids vers texte.\n",
        "# Gestion des tokens spéciaux.\n",
        "# Troncature/padding automatique.\n",
        "\n",
        "class FrenchTokenizer():\n",
        "\n",
        "# This is just a wrapper on top of the trained tokenizer to put together all the functionality we need\n",
        "# for encoding and decoding\n",
        "\n",
        "  def __init__(self, path_to_vocab, truncate=False, max_length=512):#trunc: tronquera les séquences trop longues à max_length.\n",
        "\n",
        "    self.path_to_vocab = path_to_vocab#chemin vers le fichier du tokenizer sauvegardé\n",
        "    self.tokenizer = self.prepare_tokenizer()#methode (future): charger le tokenizer depuis le fichier JSON, et retourne l’objet tokenizer.\n",
        "    self.vocab_size = len(self.tokenizer.get_vocab())#taille du vocab\n",
        "    self.special_tokens_dict = {\"[UNK]\": self.tokenizer.token_to_id(\"[UNK]\"),\n",
        "                                \"[PAD]\": self.tokenizer.token_to_id(\"[PAD]\"),\n",
        "                                \"[BOS]\": self.tokenizer.token_to_id(\"[BOS]\"),#to put at beginning of sentence\n",
        "                                \"[EOS]\": self.tokenizer.token_to_id(\"[EOS]\")}#to put at end of sentence\n",
        "\n",
        "    self.post_processor = TemplateProcessing(#TemplateProcessing définit une structure (template)\n",
        "            single=\"[BOS] $A [EOS]\",         #que le tokenizer applique après la tokenisation brute, pour\n",
        "            special_tokens=[                 #ajouter les bons tokens spéciaux selon une logique précise (single, pair).\n",
        "                (\"[EOS]\", self.tokenizer.token_to_id(\"[EOS]\")),\n",
        "                (\"[BOS]\", self.tokenizer.token_to_id(\"[BOS]\"))\n",
        "            ]\n",
        "        )#Il configure le post-processing du tokenizer, c’est-à-dire ce qui se passe après\n",
        "         # la tokenisation, mais avant l’entrée dans le modèle.\n",
        "         #En particulier ici : ajout automatique de tokens spéciaux [BOS] et [EOS] autour de la séquence.\n",
        "         #Les modèles comme les seq2seq, les encodeurs-décodeurs, ou les modèles autoregressifs\n",
        "         #ont souvent besoin de tokens [BOS]/[EOS].\n",
        "\n",
        "    self.truncate = truncate\n",
        "    if self.truncate:\n",
        "        self.max_len = max_length - self.post_processor.num_special_tokens_to_add(is_pair=False)\n",
        "        # max_length est la longueur totale autorisée en entrée du modèle (ex : 512 pour BERT).\n",
        "        # Or, ton tokenizer ajoute automatiquement des tokens spéciaux ([BOS], [EOS]) via post_processor.\n",
        "        # Ces tokens prennent de la place dans la séquence encodée.\n",
        "        # Donc, tu dois réserver cette place en enlevant leur nombre du max_length.\n",
        "\n",
        "  def prepare_tokenizer(self): #charger et configurer le tokenizer\n",
        "      tokenizer = Tokenizer.from_file(self.path_to_vocab)#load from  json file (via Tokenizer method)\n",
        "      tokenizer.decoder = decoders.WordPiece()#what decoder to use? => WordPiece\n",
        "      return tokenizer  # decode est une méthode simple, qui ne nécessite pas de configuration spécifique,\n",
        "                        # donc on l'inclut dans prepare\n",
        "                        # À l’inverse, encode dépend du contexte (troncature, tokens spéciaux, format\n",
        "                        # de retour...), donc il vaut mieux la gérer dans une méthode personnalisée\n",
        "                        # Meme si in fine on peut (et on va) implémenter un decode() spécifique\n",
        "\n",
        "\n",
        "\n",
        "# Implémentons encode et decode, qui forment l'interface principale pour convertir entre :\n",
        "# Texte ↔︎ Séquences de tokens (ids)\n",
        "# Cette interface gère aussi bien les cas individuels (une phrase) que les cas en batch (plusieurs phrases)\n",
        "# Intègre les options comme la troncature, l’ajout automatique de tokens spéciaux ([BOS], [EOS])\n",
        "# Simplifie l'utilisation pour un utilisateur final\n",
        "\n",
        "  def encode(self, input):\n",
        "    # Applique la troncature si elle est activée\n",
        "    # Applique le post-processing : ajout des [BOS], [EOS] (via TemplateProcessing)\n",
        "    # Extrait les IDs finaux (ce qu'on veut au final)\n",
        "    # Pourquoi une fonction interne ?\n",
        "    # Pour éviter de répéter le code entre les deux cas (texte simple ou batch)\n",
        "    # Elle n’est utilisée que dans encode(), donc elle est définie à l’intérieur :\n",
        "    # c’est une bonne pratique (scope local, lisibilité)\n",
        "\n",
        "    def _parse_process_tokenized(tokenized):\n",
        "        if self.truncate:\n",
        "            tokenized.truncate(self.max_len, direction=\"right\")\n",
        "        tokenized = self.post_processor.process(tokenized)\n",
        "        return tokenized.ids\n",
        "\n",
        "    if isinstance(input, str):# isinstance teste input pour voir s'il c'est une chaine de caracteres\n",
        "        tokenized = self.tokenizer.encode(input)\n",
        "        tokenized = _parse_process_tokenized(tokenized)\n",
        "\n",
        "    elif isinstance(input, (list, tuple)):#teste si c'est soit une liste soit un tuple\n",
        "        tokenized = self.tokenizer.encode_batch(input)#built-in method() from tokenizers\n",
        "        tokenized = [_parse_process_tokenized(t) for t in tokenized]\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "  def decode(self, input, skip_special_tokens=True):\n",
        "      if isinstance(input, list):\n",
        "\n",
        "          if all(isinstance(item, list) for item in input):#teste si tous les elements de inputs sont des listes\n",
        "              decoded = self.tokenizer.decode_batch(input, skip_special_tokens=skip_special_tokens)\n",
        "          elif all(isinstance(item, int) for item in input):\n",
        "              decoded = self.tokenizer.decode(input, skip_special_tokens=skip_special_tokens)\n",
        "      return decoded\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UkK8OpfK2SrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.decoder vs decode() method:\n",
        "#tokenizer.decoder = ...\tDéfinit comment recoller les morceaux de texte à partir des sous-mots\n",
        "#FrenchTokenizer.decode()\tImplémente la logique complète de décodage (selon les cas d'entrée, options, etc.)"
      ],
      "metadata": {
        "id": "EI12DbjctEwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na71tkzmrxxZ",
        "outputId": "6bdf77e0-dd1f-4996-b6e5-78cfc28a5a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instance\n",
        "path_to_data = \"/content/drive/Othercomputers/Mon ordinateur portable/Data Science/French_tokenizer/training_europarl\"\n",
        "os.listdir(path_to_data)# this is not french data so we need to upload real french data later, but now it's just for testing\n",
        "\n",
        "french_tokenizer(path_to_data)\n",
        "# fr_tok = FrenchTokenizer(path_to_data)\n"
      ],
      "metadata": {
        "id": "RH8HbIex2Sol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_vocab = \"/content/trained_tokenizer/french_wp.json\""
      ],
      "metadata": {
        "id": "CNJ4pbmm2Sg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_fr = FrenchTokenizer(path_to_vocab=path_to_vocab)\n"
      ],
      "metadata": {
        "id": "zVpVFZLy2SeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_fr.encode(\"La médiathèque est tellement bruyante\")\n"
      ],
      "metadata": {
        "id": "smkNOOEK2Sbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "434bfc23-5df6-4414-816a-bc005d64ae5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 986, 10492, 10982, 1687, 290, 3583, 21706, 237, 986, 2899, 938, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_fr.encode([\"la média est nulle\",\"l'arsenal c'est beaucoup mieux\"])\n",
        "# ca marche super bien!"
      ],
      "metadata": {
        "id": "yamieTb62SZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd67a2b-be75-4895-fe3a-e5bc14c44d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 986, 10492, 10982, 239, 1205, 20489, 237, 3],\n",
              " [2, 52, 10, 16389, 43, 10, 1205, 22226, 2301, 5250, 9027, 31932, 276, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_fr.decode([[2, 986, 10492, 10982, 239, 1205, 41, 600, 2491, 3],[2,52, 10, 16389, 43, 10, 1205, 22226,239, 1205, 20489, 237,3]], skip_special_tokens=False)"
      ],
      "metadata": {
        "id": "VvCMjy7O2SWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a66e01-7152-4952-d0a1-4b8cb24c94aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[BOS] la média est a chier [EOS]',\n",
              " \"[BOS] l ' arsenal c ' est beaua est nulle [EOS]\"]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BFTXvZZ42SRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMnYgMrR2PRj"
      },
      "outputs": [],
      "source": []
    }
  ]
}
